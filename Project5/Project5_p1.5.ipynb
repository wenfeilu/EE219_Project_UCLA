{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Popularity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "filenames = ['tweet_data/tweets_#gohawks.txt', \n",
    "             'tweet_data/tweets_#gopatriots.txt',\n",
    "             'tweet_data/tweets_#nfl.txt',\n",
    "             'tweet_data/tweets_#patriots.txt',\n",
    "             'tweet_data/tweets_#sb49.txt',\n",
    "             'tweet_data/tweets_#superbowl.txt']\n",
    "\n",
    "# aggregate the training data\n",
    "with open('tweet_data/output_file.txt', 'w',encoding='utf-8') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname,encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "# extract features\n",
    "time_tweet = []\n",
    "num_retweets = []\n",
    "num_followers = []\n",
    "num_url_citations = []\n",
    "author_names = []\n",
    "\n",
    "ranking_scores = []\n",
    "num_hashtags = []\n",
    "\n",
    "num_replies=[]\n",
    "num_impressions=[]\n",
    "num_favorite=[]\n",
    "\n",
    "\n",
    "\n",
    "with open('tweet_data/output_file.txt',encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            data = json.loads(line)\n",
    "            time_tweet.append(data['citation_date'])\n",
    "            #num_retweets.append(data['metrics']['citations']['total'])\n",
    "            num_followers.append(data['author']['followers'])\n",
    "\n",
    "            author_name = data['author']['nick']\n",
    "            original_author_name = data['original_author']['nick']\n",
    "            if author_name != original_author_name:\n",
    "                num_retweets.append(True)\n",
    "            else:\n",
    "                num_retweets.append(False)\n",
    "            num_url_citations.append(len(data['tweet']['entities']['urls']))\n",
    "            author_names.append(author_name)\n",
    "           \n",
    "            ranking_scores.append(data['metrics']['ranking_score'])\n",
    "            num_hashtags.append(data['title'].count('#'))\n",
    "            num_replies.append(data['metrics']['citations']['replies'])\n",
    "            num_impressions.append(data['metrics']['impressions'])\n",
    "            num_favorite.append(data['tweet']['favorite_count'])\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        input_file.close()\n",
    "\n",
    "# extract features\n",
    "start_time = 1421222400\n",
    "hours_passed = int((max(time_tweet)-start_time)/3600)+1 \n",
    "hourly_num_tweets = [0] * hours_passed\n",
    "hourly_num_retweets = [0] * hours_passed\n",
    "hourly_tot_followers = [0] * hours_passed\n",
    "\n",
    "hourly_time_day = [0] * hours_passed\n",
    "totnum_tweets = float(len(time_tweet))\n",
    "hourly_num_url_citations = [0] * hours_passed\n",
    "hourly_num_authors = [0] * hours_passed\n",
    "hourly_author_set = [0] * hours_passed\n",
    "for i in range(0, hours_passed):\n",
    "    hourly_author_set[i] = set([])\n",
    "\n",
    "hourly_tot_ranking_scores = [0.0] * hours_passed\n",
    "hourly_num_hashtags = [0] * hours_passed\n",
    "\n",
    "hourly_number_of_impressions=[0] * hours_passed\n",
    "hourly_number_of_replies=[0] * hours_passed\n",
    "hourly_number_of_favorite_count=[0] * hours_passed\n",
    "\n",
    "\n",
    "for i in range(int(totnum_tweets)):\n",
    "    current_hour = int((time_tweet[i]-start_time)/3600)\n",
    "\n",
    "    hourly_num_tweets[current_hour] += 1\n",
    "    if num_retweets[i]:\n",
    "        hourly_num_retweets[current_hour] += 1\n",
    "\n",
    "    hourly_tot_followers[current_hour] += num_followers[i]\n",
    "\n",
    " \n",
    "    hourly_num_url_citations[current_hour] += num_url_citations[i]\n",
    "    hourly_author_set[current_hour].add(author_names[i])\n",
    "   \n",
    "    hourly_tot_ranking_scores[current_hour] += ranking_scores[i]\n",
    "    hourly_num_hashtags[current_hour] += num_hashtags[i]\n",
    "    \n",
    "    hourly_number_of_impressions[current_hour] +=num_impressions[i]\n",
    "    hourly_number_of_replies[current_hour] +=num_replis[i]\n",
    "    hourly_number_of_favorite_count[current_hour] +=num_favorite[i]\n",
    "\n",
    "        \n",
    "\n",
    "    for i in range(0, len(hourly_author_set)):\n",
    "        hourly_num_authors[i] = len(hourly_author_set[i])\n",
    "    for i in range(len(hourly_time_day)):\n",
    "        hourly_time_day[i] = i%24\n",
    "        \n",
    "# transfer features from one hour to five hour\n",
    "five_hour_num_tweets = []\n",
    "five_hour_num_retweets = []\n",
    "five_hour_tot_followers = []\n",
    "five_hour_maxnum_followers = []\n",
    "five_hour_time_day = []\n",
    "five_hour_num_url_citations = []\n",
    "five_hour_num_authors = []\n",
    "\n",
    "five_hour_tot_ranking_scores = []\n",
    "five_hour_num_hashtags = []\n",
    "\n",
    "five_hour_num_impressions=[]\n",
    "five_hour_num_replies=[]\n",
    "five_hour_num_favorite=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(hours_passed):\n",
    "    five_hour_num_tweets.append(sum(hourly_num_tweets[i:i+5]))\n",
    "    five_hour_num_retweets.append(sum(hourly_num_retweets[i:i+5]))\n",
    "    five_hour_tot_followers.append(sum(hourly_tot_followers[i:i+5]))\n",
    "    \n",
    "    five_hour_time_day.append(sum(hourly_time_day[i:i+5]))\n",
    "    five_hour_num_url_citations.append(sum(hourly_num_url_citations[i:i+5]))\n",
    "    five_hour_num_authors.append(sum(hourly_num_authors[i:i+5]))\n",
    "    \n",
    "    five_hour_tot_ranking_scores.append(sum(hourly_tot_ranking_scores[i:i+5]))\n",
    "    five_hour_num_hashtags.append(sum(hourly_num_hashtags[i:i+5]))\n",
    "    \n",
    "    \n",
    "    five_hour_num_impressions.append(sum(hourly_number_of_impressions[i:i+5]))\n",
    "    five_hour_num_replies.append(sum(hourly_number_of_replies[i:i+5]))\n",
    "    five_hour_num_favorite.append(sum(hourly_number_of_favorite_count[i:i+5]))\n",
    "    \n",
    "    \n",
    "five_hour_num_tweets = five_hour_num_tweets[:-4]\n",
    "five_hour_num_retweets = five_hour_num_retweets[:-4]\n",
    "five_hour_tot_followers = five_hour_tot_followers[:-4]\n",
    "\n",
    "five_hour_time_day = five_hour_time_day[:-4]\n",
    "five_hour_num_url_citations = five_hour_num_url_citations[:-4]\n",
    "five_hour_num_authors = five_hour_num_authors[:-4]\n",
    "\n",
    "five_hour_tot_ranking_scores = five_hour_tot_ranking_scores[:-4]\n",
    "five_hour_num_hashtags = five_hour_num_hashtags[:-4]\n",
    "\n",
    "\n",
    "five_hour_num_impressions=five_hour_num_impressions[:-4]\n",
    "five_hour_num_replies= five_hour_num_replies[:-4]\n",
    "five_hour_num_favorite=five_hour_num_favorite[:-4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build DataFrame and save to file\n",
    "target_value = hourly_num_tweets[5:]\n",
    "target_value.append(0)\n",
    "data = np.array([five_hour_num_tweets,\n",
    "                 five_hour_num_retweets,\n",
    "                 five_hour_tot_followers,\n",
    "                 five_hour_time_day,\n",
    "                 five_hour_num_url_citations,\n",
    "                 five_hour_num_authors,\n",
    "                 five_hour_tot_ranking_scores,\n",
    "                 five_hour_num_hashtags,\n",
    "                 five_hour_num_impressions,\n",
    "                 five_hour_num_replies,\n",
    "                 five_hour_num_favorite,\n",
    "                 target_value])\n",
    "data = np.transpose(data)\n",
    "df = DataFrame(data)\n",
    "df.columns = ['num_tweets',\n",
    "              'num_retweets',\n",
    "              'sum_followers',\n",
    "              'time_of_day', \n",
    "              'num_URLs', \n",
    "              'num_authors', \n",
    "              'ranking_score', \n",
    "              'num_hashtags',\n",
    "              'num_impressions',\n",
    "              'num_replies',\n",
    "              'num_favorite',\n",
    "              'target_value']\n",
    "if os.path.isdir('./Extracted_data'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('./Extracted_data')\n",
    "df.to_csv('./Extracted_data/Q5.csv', index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction error before Feb. 1, 8:00 a.m.:\n",
      "5470.98762751\n",
      "Average prediction error between Feb. 1, 8:00 a.m. and 8:00 p.m.:\n",
      "81748.2161516\n",
      "Average prediction error after Feb. 1, 8:00 p.m.:\n",
      "757.530975068\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#------------------linear regression-----------------------------------------\n",
    "def average_prediction_error(target_data, cross_predicted_values):\n",
    "    total_error = 0.0\n",
    "    for (actual, predicted) in zip(target_data, cross_predicted_values):\n",
    "        total_error += abs(actual - predicted)\n",
    "    print(total_error/len(target_data))\n",
    "    \n",
    "training_data = pd.read_csv('./Extracted_data/Q5.csv')\n",
    "target_data = training_data.pop('target_value')\n",
    "    \n",
    "training_data1 = training_data[:440] # Before Feb. 1, 8:00 a.m.\n",
    "training_data2 = training_data[440:452] # Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
    "training_data3 = training_data[452:] # After Feb. 1, 8:00 p.m. \n",
    "\n",
    "target_data1 = target_data[:440]\n",
    "target_data2 = target_data[440:452]\n",
    "target_data3 = target_data[452:]  \n",
    "\n",
    "lin_reg = LinearRegression(fit_intercept = False)\n",
    "cross_predicted_values1 = cross_val_predict(lin_reg, training_data1, target_data1, cv = 10)\n",
    "cross_predicted_values2 = cross_val_predict(lin_reg, training_data2, target_data2, cv = 10)\n",
    "cross_predicted_values3 = cross_val_predict(lin_reg, training_data3, target_data3, cv = 10)\n",
    "print('Average prediction error before Feb. 1, 8:00 a.m.:')\n",
    "average_prediction_error(target_data1,cross_predicted_values1)\n",
    "\n",
    "print('Average prediction error between Feb. 1, 8:00 a.m. and 8:00 p.m.:')\n",
    "average_prediction_error(target_data2,cross_predicted_values2)\n",
    "\n",
    "print('Average prediction error after Feb. 1, 8:00 p.m.:')\n",
    "average_prediction_error(target_data3,cross_predicted_values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction error before Feb. 1, 8:00 a.m.:\n",
      "1922.00670071\n",
      "Average prediction error between Feb. 1, 8:00 a.m. and 8:00 p.m.:\n",
      "87542.4708333\n",
      "Average prediction error after Feb. 1, 8:00 p.m.:\n",
      "656.249490571\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------random forest-------------------------------------------\n",
    "rf = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "cross_predicted_values1 = cross_val_predict(rf, training_data1, target_data1, cv = 10)\n",
    "cross_predicted_values2 = cross_val_predict(rf, training_data2, target_data2, cv = 10)\n",
    "cross_predicted_values3 = cross_val_predict(rf, training_data3, target_data3, cv = 10)\n",
    "print('Average prediction error before Feb. 1, 8:00 a.m.:')\n",
    "average_prediction_error(target_data1,cross_predicted_values1)\n",
    "\n",
    "print('Average prediction error between Feb. 1, 8:00 a.m. and 8:00 p.m.:')\n",
    "average_prediction_error(target_data2,cross_predicted_values2)\n",
    "\n",
    "print('Average prediction error after Feb. 1, 8:00 p.m.:')\n",
    "average_prediction_error(target_data3,cross_predicted_values3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above, we will choose random forest model to predict test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['test_data/sample1_period1.txt', \n",
    "             'test_data/sample2_period2.txt',\n",
    "             'test_data/sample3_period3.txt',\n",
    "             'test_data/sample4_period1.txt',\n",
    "             'test_data/sample5_period1.txt',\n",
    "             'test_data/sample6_period2.txt',\n",
    "             'test_data/sample7_period3.txt',\n",
    "             'test_data/sample8_period1.txt',\n",
    "             'test_data/sample9_period2.txt',\n",
    "             'test_data/sample10_period3.txt']\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "rf_before = rf.fit(training_data1, target_data1)\n",
    "rf_during = rf.fit(training_data2, target_data2)\n",
    "rf_after = rf.fit(training_data3, target_data3)\n",
    "\n",
    "def predict_tweet(filename):\n",
    "    time_tweet = []\n",
    "    num_retweets = []\n",
    "    num_followers = []\n",
    "    num_url_citations = []\n",
    "    author_names = []\n",
    "    ranking_scores = []\n",
    "    num_hashtags = []\n",
    "    \n",
    "    num_replies=[]\n",
    "    num_impressions=[]\n",
    "    num_favorite=[]\n",
    "\n",
    "    with open('test_data/'+filename,encoding='utf-8') as input_file:\n",
    "            for line in input_file:\n",
    "                data = json.loads(line)\n",
    "                time_tweet.append(data['citation_date'])\n",
    "                #num_retweets.append(data['metrics']['citations']['total'])\n",
    "                num_followers.append(data['author']['followers'])\n",
    "\n",
    "                author_name = data['author']['nick']\n",
    "                original_author_name = data['original_author']['nick']\n",
    "                if author_name != original_author_name:\n",
    "                    num_retweets.append(True)\n",
    "                else:\n",
    "                    num_retweets.append(False)\n",
    "                num_url_citations.append(len(data['tweet']['entities']['urls']))\n",
    "                author_names.append(author_name)\n",
    "                ranking_scores.append(data['metrics']['ranking_score'])\n",
    "                num_hashtags.append(data['title'].count('#'))\n",
    "                \n",
    "                num_replies.append(data['metrics']['citations']['replies'])\n",
    "                num_impressions.append(data['metrics']['impressions'])\n",
    "                num_favorite.append(data['tweet']['favorite_count'])\n",
    "\n",
    "                \n",
    "            \n",
    "\n",
    "            input_file.close()\n",
    "\n",
    "    # extract features\n",
    "    start_time = 1421222400\n",
    "    hours_passed = int((max(time_tweet)-start_time)/3600)+1 \n",
    "    hourly_num_tweets = [0] * hours_passed\n",
    "    hourly_num_retweets = [0] * hours_passed\n",
    "    hourly_tot_followers = [0] * hours_passed\n",
    "    hourly_time_day = [0] * hours_passed\n",
    "    totnum_tweets = float(len(time_tweet))\n",
    "    hourly_num_url_citations = [0] * hours_passed\n",
    "    hourly_num_authors = [0] * hours_passed\n",
    "    hourly_author_set = [0] * hours_passed\n",
    "    for i in range(0, hours_passed):\n",
    "        hourly_author_set[i] = set([])\n",
    "    hourly_tot_ranking_scores = [0.0] * hours_passed\n",
    "    hourly_num_hashtags = [0] * hours_passed\n",
    "    \n",
    "    \n",
    "    hourly_number_of_impressions=[0] * hours_passed\n",
    "    hourly_number_of_replies=[0] * hours_passed\n",
    "    hourly_number_of_favorite_count=[0] * hours_passed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(int(totnum_tweets)):\n",
    "        current_hour = int((time_tweet[i]-start_time)/3600)\n",
    "\n",
    "        hourly_num_tweets[current_hour] += 1\n",
    "        if num_retweets[i]:\n",
    "            hourly_num_retweets[current_hour] += 1\n",
    "\n",
    "        hourly_tot_followers[current_hour] += num_followers[i]\n",
    "\n",
    "        hourly_num_url_citations[current_hour] += num_url_citations[i]\n",
    "        hourly_author_set[current_hour].add(author_names[i])\n",
    "        \n",
    "        hourly_tot_ranking_scores[current_hour] += ranking_scores[i]\n",
    "        hourly_num_hashtags[current_hour] += num_hashtags[i]\n",
    "        \n",
    "        \n",
    "        hourly_number_of_impressions[current_hour] +=num_impressions[i]\n",
    "        hourly_number_of_replies[current_hour] +=num_replies[i]\n",
    "        hourly_number_of_favorite_count[current_hour] +=num_favorite[i]\n",
    "\n",
    "\n",
    "        for i in range(0, len(hourly_author_set)):\n",
    "            hourly_num_authors[i] = len(hourly_author_set[i])\n",
    "        for i in range(len(hourly_time_day)):\n",
    "            hourly_time_day[i] = i%24\n",
    "\n",
    "    # transfer features from one hour to six hour\n",
    "    six_hour_num_tweets = []\n",
    "    six_hour_num_retweets = []\n",
    "    six_hour_tot_followers = []\n",
    "\n",
    "    six_hour_time_day = []\n",
    "    six_hour_num_url_citations = []\n",
    "    six_hour_num_authors = []\n",
    "    \n",
    "    six_hour_tot_ranking_scores = []\n",
    "    six_hour_num_hashtags = []\n",
    "    \n",
    "    six_hour_num_impressions=[]\n",
    "    six_hour_num_replies=[]\n",
    "    six_hour_num_favorite=[]\n",
    "    \n",
    "    six_hour_num_tweets.append(sum(hourly_num_tweets))\n",
    "    six_hour_num_retweets.append(sum(hourly_num_retweets))\n",
    "    six_hour_tot_followers.append(sum(hourly_tot_followers))\n",
    "    \n",
    "    six_hour_time_day.append(sum(hourly_time_day))\n",
    "    six_hour_num_url_citations.append(sum(hourly_num_url_citations))\n",
    "    six_hour_num_authors.append(sum(hourly_num_authors))\n",
    " \n",
    "    six_hour_tot_ranking_scores.append(sum(hourly_tot_ranking_scores))\n",
    "    six_hour_num_hashtags.append(sum(hourly_num_hashtags))\n",
    "    \n",
    "    six_hour_num_impressions.append(sum(hourly_number_of_impressions))\n",
    "    \n",
    "    six_hour_num_replies.append(sum(hourly_number_of_replies))\n",
    "    six_hour_num_favorite.append(sum(hourly_number_of_favorite_count))\n",
    "    \n",
    "#     six_hour_num_tweets = six_hour_num_tweets[:-5]\n",
    "#     six_hour_num_retweets = six_hour_num_retweets[:-5]\n",
    "#     six_hour_tot_followers = six_hour_tot_followers[:-5]\n",
    "#     six_hour_maxnum_followers = six_hour_maxnum_followers[:-5]\n",
    "#     six_hour_time_day = six_hour_time_day[:-5]\n",
    "#     six_hour_num_url_citations = six_hour_num_url_citations[:-5]\n",
    "#     six_hour_num_authors = six_hour_num_authors[:-5]\n",
    "#     six_hour_num_mentions = six_hour_num_mentions[:-5]\n",
    "#     six_hour_tot_ranking_scores = six_hour_tot_ranking_scores[:-5]\n",
    "#     six_hour_num_hashtags = six_hour_num_hashtags[:-5]\n",
    "\n",
    "    # Build DataFrame and save to file\n",
    "#     target_value = hourly_num_tweets[6:]\n",
    "#     target_value.append(0)\n",
    "    data = np.array([six_hour_num_tweets,\n",
    "                     six_hour_num_retweets,\n",
    "                     six_hour_tot_followers,\n",
    "                     six_hour_time_day,\n",
    "                     six_hour_num_url_citations,\n",
    "                     six_hour_num_authors,\n",
    "                     six_hour_tot_ranking_scores,\n",
    "                     six_hour_num_hashtags,\n",
    "                     six_hour_num_impressions,\n",
    "                     six_hour_num_replies,\n",
    "                     six_hour_num_favorite\n",
    "                     ])\n",
    "    data = np.transpose(data)\n",
    "    df = DataFrame(data)\n",
    "    df.columns = ['num_tweets',\n",
    "                  'num_retweets',\n",
    "                  'sum_followers',\n",
    "                  'time_of_day', \n",
    "                  'num_URLs', \n",
    "                  'num_authors',\n",
    "                  'ranking_score', \n",
    "                  'num_hashtags', \n",
    "                  'num_impressions',\n",
    "                  'num_replies',\n",
    "                  'num_favorite'\n",
    "                  ]\n",
    "    if os.path.isdir('./Extracted_data'):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir('./Extracted_data')\n",
    "    df.to_csv('./Extracted_data/Q5_'+filename[:-4]+'.csv', index = False)\n",
    "    \n",
    "    testing_data = pd.read_csv('./Extracted_data/Q5_'+filename[:-4]+'.csv')\n",
    "#     target_data = testing_data.pop('target_value')\n",
    "    predicted_y = []\n",
    "    if filename[-5] == '1':\n",
    "        predicted_y = rf_before.predict(testing_data)\n",
    "        print(predicted_y)\n",
    "    elif filename[-5] == '2':\n",
    "        predicted_y = rf_during.predict(testing_data)\n",
    "        print(predicted_y)\n",
    "    else:\n",
    "        predicted_y = rf_after.predict(testing_data)\n",
    "        print(predicted_y)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 475.95]\n",
      "[ 4485.55]\n",
      "[ 539.125]\n",
      "[ 472.7]\n",
      "[ 493.55]\n",
      "[ 4728.4]\n",
      "[ 36.8]\n",
      "[ 66.95]\n",
      "[ 1689.965]\n",
      "[ 53.65]\n"
     ]
    }
   ],
   "source": [
    "predict_tweet('sample1_period1.txt')\n",
    "predict_tweet('sample2_period2.txt')\n",
    "predict_tweet('sample3_period3.txt')\n",
    "predict_tweet('sample4_period1.txt')\n",
    "predict_tweet('sample5_period1.txt')\n",
    "predict_tweet('sample6_period2.txt')\n",
    "predict_tweet('sample7_period3.txt')\n",
    "predict_tweet('sample8_period1.txt')\n",
    "predict_tweet('sample9_period2.txt')\n",
    "predict_tweet('sample10_period3.txt')\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
